{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1079953,"sourceType":"datasetVersion","datasetId":601280}],"dockerImageVersionId":30357,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Lung and Colon Histopathology Classification using ResNet-50 V2\n","metadata":{}},{"cell_type":"markdown","source":"The purpose of this notebook is to train an image classification model to identify carcinomas in histopathological images. \n\nThe model has been trained in the [Lung and Colon Cancer Histopathological Images](https://www.kaggle.com/datasets/andrewmvd/lung-and-colon-cancer-histopathological-images) dataset. The model identifies three different classes: `Benign`, `Adenocarcinoma`, `Squamous Cell Carcinoma`. The idea behind this decision is to make a model capable of generalize the diagnostic regardless the organ of the sample. \n\nIn order to improve the accuracy of the model, I have used transfer learning with the [ResNet-50 V2](https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5) model trained on Imagenet. This decision is based on the article [Pathologist-level Classification of Histologic Patterns on Resected Lung Adenocarcinoma Slides with Deep Neural Networks](https://arxiv.org/pdf/1901.11489v1.pdf) written by Jason W. Wei et al., where the authors recommend to use `ResNet-18` trained with Imagenet dataset to classify histopathological images. ","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup\n","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom contextlib import ExitStack\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import (\n    Sequential,\n    layers,\n    optimizers,\n    regularizers,\n    callbacks,\n    metrics,\n    losses,\n    activations,\n)\nimport tensorflow_hub as hub\nfrom tensorflow.train import Example, Feature, Features, BytesList, Int64List, FloatList","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 256\nAUTOTUNE = tf.data.AUTOTUNE\nSHUFFLE_BUFFER = 1000\nDATASET_PATH = tf.io.gfile.join(os.environ[\"DATA_PATH\"], \"lung_colon_histopathology\")\nRAW_DATASET_PATH = tf.io.gfile.join(DATASET_PATH, \"raw_data\")\nTFRECORDS_PATH = tf.io.gfile.join(DATASET_PATH, \"tfrecord_data\")\nMODEL_PATH = os.path.join(\"..\", \"..\", \"..\", \"models\", \"chapter_14\", \"lung_colon_histopathology\")\nSEED = 1992\n\n# List of all the diagnostics\nTYPE_TISSUE = {\n    0: \"Benign\",\n    1: \"Adenocarcinoma\",\n    2: \"Squamous Cell Carcinoma\",\n}\n\nNAME_CLASSES = [\"Benign\", \"Adenocarcinoma\", \"Squamous Cell Carcinoma\"]\n\nIMG_SIZE = (768, 768)\nIMG_CHANNELS = 3\nNEW_IMG_SIZE = [448, 448, 3]\n\n# Model Url\nMODEL_URL = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.1 Helper Functions","metadata":{}},{"cell_type":"code","source":"def exponential_decay_with_warmup(\n    lr_start=1e-4,\n    lr_max=1e-3,\n    lr_min=1e-5,\n    lr_rampup_epochs=4,\n    lr_sustain_epochs=1,\n    lr_exp_decay=0.8,\n):\n    \"\"\"Implements exponential decay learning rate with warm up.\n    \n    example:\n        lr_function = exponential_decay_wtih_warmup()\n        lr_cb = tf.keras.callbacks.LearningRateScheduler(lr_function)\n\n    Args:\n        lr_start (float, optional): Initial value of the learning rate. Defaults to 0.0001.\n        lr_max (float, optional): Maximum value of the learning rate. Defaults to 0.0001.\n        lr_min (float, optional): Minimum value of the learning rate. Defaults to 0.00001.\n        lr_rampup_epochs (int, optional): Number of epochs that the learning rate will increase up to lr_max. Defaults to 4.\n        lr_sustain_epochs (int, optional): Number of epochs the learning rate will be equal to lr_max. Defaults to 1.\n        lr_exp_decay (float, optional): Factor in which the learning rate will decay. Defaults to 0.8.\n    \"\"\"\n\n    def exponential_decay_fn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < (lr_rampup_epochs + lr_sustain_epochs):\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay ** (\n                epoch - lr_rampup_epochs - lr_sustain_epochs\n            ) + lr_min\n\n        return lr\n\n    return exponential_decay_fn","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def balanced_split(dataset, percentages=[0.80, 0.10, 0.10], verbose=False):\n    \"\"\"\n    Split a given dataset into three datasets, defined by percentages, according to the classes in the dataset.\n\n    Args:\n        dataset (tf.data.Dataset): The dataset to be splitted.\n        percentages (List[float], optional): A list with 3 elements that defines the percentage of the dataset\n            that will be used for each of the sets. The elements must be floats in the range [0, 1] and\n            must sum up to 1. Defaults to [0.80, 0.10, 0.10].\n        verbose (bool, optional): A boolean that defines whether the function will print the split for each\n            class and the final split. Defaults to False.\n\n    Returns:\n        tuple: A tuple with three datasets, corresponding to the training, validation and testing sets, respectively.\n    \"\"\"\n    # Obtain the different classes in the datasets and sort the list\n    list_classes = dataset.map(lambda x, y: y, num_parallel_calls=_AUTOTUNE).unique()\n    list_classes = [class_.numpy() for class_ in list_classes]\n\n    # Initialize the sets to False. This is just to avoid creating\n    # a dataset without knowing the dimensions. This will not affect\n    # the final dataset since the variable will be totally overwritten\n    train_set = False\n    valid_set = False\n    test_set = False\n\n    # Keep track of the total samples per set\n    samples_train = 0\n    samples_valid = 0\n    samples_test = 0\n\n    for class_ in list_classes:\n        # Get the samples that match every class and samples per set\n        tmp_dataset = dataset.filter(lambda x, y: y == class_)\n        n_samples = len(list(tmp_dataset.as_numpy_iterator()))\n\n        n_valid = int(percentages[1] * n_samples)\n        n_test = int(percentages[1] * n_samples)\n        n_train = n_samples - n_valid - n_test\n\n        samples_train += n_train\n        samples_valid += n_valid\n        samples_test += n_test\n\n        # Separate the sets and concatenate to the other classes sets\n        tmp_train_set = tmp_dataset.take(n_train)\n        tmp_valid_set = tmp_dataset.skip(n_train).take(n_valid)\n        tmp_test_set = tmp_dataset.skip(n_train).skip(n_valid)\n\n        train_set = (\n            tmp_train_set\n            if train_set == False\n            else train_set.concatenate(tmp_train_set)\n        )\n        valid_set = (\n            tmp_valid_set\n            if valid_set == False\n            else valid_set.concatenate(tmp_valid_set)\n        )\n        test_set = (\n            tmp_test_set if test_set == False else test_set.concatenate(tmp_test_set)\n        )\n\n        if verbose == True:\n            print(f\"\\tSplit for class {class_} is [{n_train}, {n_valid}, {n_test}]\")\n\n    if verbose == True:\n        print(\"\\nThe Split has been completed. The final split is the following: \")\n        print(\n            f\"\\tTraining Set: {samples_train}\\n\\tValidation Set:{samples_valid}\\n\\tTesting Set:{samples_valid}\"\n        )\n    return train_set, valid_set, test_set","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_logdir(date_type=\"date\", path_folder=None, id_value=None):\n    \"\"\"This function creates the name of a folder for Tensorboard\n    logs using the current date or datetime\n\n    Args:\n        date_type (str, optional): Format of the second part of the folder name. Options \"date\", \"datetime\", \"id\".\n            If selected \"id\", argument id must be provided. Defaults to \"date\".\n        path_folder (str, optional): String of the path to add before the folder name. Defaults to None.\n        id (str, optional): String use to create the name of the folder if option \"id\" has been selected.\n            Defaults to None.\n\n    Returns:\n        str: Name of the folder or path of the folder.\n    \"\"\"\n    log_dir = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if date_type == \"datetype\" else datetime.now().strftime(\"%Y%m%d\")\n    log_dir = f\"run_{log_dir}\" if not path_folder else os.path.join(path_folder, f\"run_{log_dir}\")\n    \n    log_id = \"\"\n    if date_type == \"date\":\n        log_id = datetime.now().strftime(\"%Y%m%d\")\n    elif date_type == \"datetime\":\n        log_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    elif date_type == \"id\":\n        log_id = id_value\n    \n    log_folder = f\"run_{log_id}\"\n    \n    if path_folder:\n        log_dir = os.path.join(path_folder, log_folder)\n    else:\n        log_dir = log_folder\n\n    return log_dir","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Data Pipeline\n","metadata":{}},{"cell_type":"markdown","source":"The images are provided in two separate folders as follows:\n<pre>\n├── colon_images\n│   ├── colon_aca\n│   └── colon_n\n└── lung_images\n    ├── lung_aca\n    ├── lung_n\n    └── lung_scc</pre>\n\nThe images are labeled according to the folders in the second level. This creates only three posible labels as mentioned above. \n\nThe first part of the data pipeline is to load the images, add the labels and save them as TFRecord files. The reason for this is that TensorFlow is more efficient when working with this files, reducing the training time. ","metadata":{}},{"cell_type":"code","source":"def get_folders_images(filepath):\n    \"\"\"\n    This function takes in a filepath as an input. It uses the TensorFlow library's\n    `tf.io.gfile.listdir()` function to list the names of all the folders located in \n    the given filepath. It then creates a list of filepaths by joining the filepath \n    with the names of the folders. It then creates a dictionary where the keys are the \n    names of the folders and the values are the corresponding filepaths.\n\n    Args:\n        filepath: the filepath where the folders are located\n\n    Returns:\n        list_folders: a dictionary where keys are the folder names and values are the \n        corresponding filepaths\n    \"\"\"\n    name_folders = tf.io.gfile.listdir(filepath)\n    path_folders = [tf.io.gfile.join(filepath, folder) for folder in name_folders]\n\n    list_folders = {folder: path for folder, path in zip(name_folders, path_folders)}\n    return list_folders\n\n\ndef create_example(image, tissue_label):\n    \"\"\"\n    This function takes in an image and a tissue label as inputs. It serializes the image\n    using TensorFlow's `tf.io.serialize_tensor()` function, and creates a feature dictionary \n    with two keys: \"image\" and \"tissue_label\".\n\n    Args:\n        image: a Tensorflow tensor representing an image\n        tissue_label: a Tensorflow tensor representing tissue label of the image\n\n    Returns:\n        example: TensorFlow Example object\n    \"\"\"\n    image_data = tf.io.serialize_tensor(image)\n    feature = {\n        \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n        \"tissue_label\": Feature(int64_list=Int64List(value=[tissue_label.numpy()])),\n    }\n\n    return Example(features=Features(feature=feature))\n\n\ndef save_protobufs(dataset, type_set=\"train\", n_shards=10):\n    \"\"\"\n    This function takes in a dataset, a type of the set (train, valid or test) and the \n    number of shards. It creates a folder with the given type_set name in the TFRECORDS_PATH \n    directory, then creates a list of filepaths by joining the the folder name of the \n    dataset and the file names. The dataset is shuffled if the type_set is train. Then \n    it creates shard number of tfRecord files in the set_folder with the given file names\n    and writes the dataset in the created files.\n\n    Args:\n        dataset: a TensorFlow Dataset\n        type_set: string, either 'train', 'valid' or 'test'\n        n_shards: number of TFRecord files to be created\n\n    Returns:\n        file_paths: a list of filepaths where the data is saved in the TFRecord format\n    \"\"\"\n    set_folder = tf.io.gfile.join(TFRECORDS_PATH, type_set)\n\n    tf.io.gfile.makedirs(set_folder)\n    file_paths = [tf.io.gfile.join(set_folder, filepath) for filepath in files]\n\n    if type_set == \"train\":\n        dataset.shuffle(SHUFFLE_BUFFER)\n    files = [\n        f\"{type_set}.tfrecord-{shard.numpy() + 1:02d}-of-{n_shards:02d}\"\n        for shard in tf.range(n_shards)\n    ]\n\n    with ExitStack() as stack:\n        writers = [\n            stack.enter_context(tf.io.TFRecordWriter(file)) for file in file_paths\n        ]\n        for index, (image, organ_label, tissue_label) in dataset.enumerate():\n            shard = index % n_shards\n            example = create_example(image, organ_label, tissue_label)\n            writers[shard].write(example.SerializeToString())\n    return file_paths\n\n\ndef get_folders_tfrecords(type_set=\"train\"):\n    \"\"\"\n    This function takes in a type of the set (train, valid or test) and returns the list \n    of filepaths of the TFRecord files located in the corresponding folder in the \n    TFRECORDS_PATH directory.\n\n    Args:\n        type_set: string, either 'train', 'valid' or 'test'\n\n    Returns:\n        list_files: a list of filepaths where the data is saved in the tfRecord format\n    \"\"\"\n    folder = tf.io.gfile.join(TFRECORDS_PATH, type_set)\n    files = tf.io.gfile.listdir(folder)\n    list_files = [tf.io.gfile.join(folder, filepath) for filepath in files]\n    return list_files\n\n\ndef save_images_protobufs():\n    \"\"\"\n    This function creates three datasets (train, valid, test) from the images located in \n    the RAW_DATASET_PATH directory, saves the datasets in tfRecord format, and returns \n    the list of filepaths where the data is saved. If the folder for the TFRecords already\n    exists, it only returns the list of filepaths.\n    \"\"\"\n    # Verify if the folder for the TFRecords exist to process the data\n    if not tf.io.gfile.exists(TFRECORDS_PATH):\n\n        tf.io.gfile.makedirs(TFRECORDS_PATH)\n\n        list_folders = get_folders_images(RAW_DATASET_PATH)\n\n        # Create the dataset per folder, split the dataset into train, valid and test sets\n        colon_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n            list_folders[\"colon_images\"], batch_size=None, image_size=IMG_SIZE\n        )\n        train_colon, valid_colon, test_colon = balanced_split(\n            colon_dataset\n        )\n\n        lung_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n            list_folders[\"lung_images\"], batch_size=None, image_size=IMG_SIZE\n        )\n        train_lung, valid_lung, test_lung = balanced_split(lung_dataset)\n\n        # Combines the datasets into a single one\n        train_set = train_colon.concatenate(train_lung)\n        valid_set = valid_colon.concatenate(valid_lung)\n        test_set = test_colon.concatenate(test_lung)\n\n        train_paths = save_protobufs(train_set, \"train\")\n        valid_paths = save_protobufs(valid_set, \"valid\")\n        test_paths = save_protobufs(test_set, \"test\")\n    else:\n        # If the folders exist, only return the list of filepaths\n        train_paths = get_folders_tfrecords(\"train\")\n        valid_paths = get_folders_tfrecords(\"valid\")\n        test_paths = get_folders_tfrecords(\"test\")\n\n    return train_paths, valid_paths, test_paths\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_files, valid_files, test_files = save_images_protobufs()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The second part of the data pipeline is loading the images and preprocessing them. \n\nThe original size of the images is `768x768`. The `ResNet-50` documentation suggest to use `224x224` size images to feed the model. Reducing this much would mean to lose a lot of spatial and resolution information. Instead of directly reducing the size to `224x224`, the images are resized to `448x448` (double the size recommended). This will allow to use a `MaxPool2D` layer to reduce the size without losing that much information of the image.","metadata":{}},{"cell_type":"code","source":"def get_record(tfrecord):\n    \"\"\"\n    This function takes in a tfrecord, parse it using TensorFlow's `tf.io.parse_single_example()` \n    function, and returns the image and tissue label as a tuple.\n    The function also parses the tensor, reshapes it and resize it to the desired size.\n    \n    Args:\n        tfrecord: a TensorFlow TFRecord\n    \n    Returns:\n        image: Tensorflow tensor representing the image\n        tissue_label: Tensorflow tensor representing the tissue label of the image\n    \"\"\"\n    feature_descriptions = {\n        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n        \"tissue_label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n    }\n\n    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.float32)\n    image = tf.reshape(image, shape=[IMG_SIZE[0], IMG_SIZE[1], IMG_CHANNELS])\n    image = tf.image.resize(image, size=[NEW_IMG_SIZE[0], NEW_IMG_SIZE[1]])\n    return image, example[\"tissue_label\"]\n\n\ndef get_dataset(file_paths, cache=False, shuffle_buffer=None):\n    \"\"\"\n    This function takes in a list of filepaths, loads the data in the tfRecord format \n    and returns a TensorFlow dataset. The function can also cache the dataset, shuffle \n    it and return it in multi-label format.\n    \n    Args:\n        file_paths: list of filepaths where the data is saved in the TFRecord format\n        cache: boolean, whether to cache the dataset or not\n        shuffle_buffer: int, the buffer size for shuffling the dataset\n    \n    Returns:\n        dataset: TensorFlow Dataset\n    \"\"\"\n    dataset = tf.data.TFRecordDataset(file_paths, num_parallel_reads=AUTOTUNE)\n\n    if cache:\n        dataset = dataset.cache()\n    if shuffle_buffer:\n        dataset = dataset.shuffle(shuffle_buffer)\n\n    dataset = (\n        dataset.map(get_record, num_parallel_calls=AUTOTUNE)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTOTUNE)\n    )\n\n    return dataset","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_set = get_dataset(train_files, shuffle_buffer=SHUFFLE_BUFFER)\nvalid_set = get_dataset(valid_files)\ntest_set = get_dataset(test_files)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Training\n","metadata":{}},{"cell_type":"code","source":"# Clear the TensorFlow session\ntf.keras.backend.clear_session()\ntf.random.set_seed(SEED)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model is trained taking into consideration the following:\n\n- The images are normalized being divided by `255.0`\n- The first layer of the model is a max pooling layer to reduce the spatial dimensionality, and match the suggested size of the images.\n- The ResNet model is not set as trainable to reduce the training time. \n- In the article mentioned above, they used the learning rate with decay. In this case, I selected learning rate with warmup and exponential decay. \n- The optimizer selected is Adam.\n","metadata":{}},{"cell_type":"code","source":"def train_model(\n    train_set,\n    valid_set,\n    learning_rate,\n    epochs=100,\n    trainable=False,\n):\n    \"\"\"\n    This function trains a ResNet model on the given train and validation sets.\n    The function normalizes the data, loads the ResNet model, compiles the model,\n    and sets up callbacks for learning rate scheduling, TensorBoard, model checkpointing, \n    and early stopping. The model is then fit on the train set and validated on the \n    validation set.\n    \n    Args:\n        train_set: TensorFlow dataset for training the model\n        valid_set: TensorFlow dataset for validation the model\n        learning_rate: float, the initial learning rate for the optimizer\n        epochs: int, the number of epochs for training the model\n        trainable: boolean, whether to make the base model trainable or not\n    \n    Returns:\n        None\n    \"\"\"\n    # Normalization of the data\n    def normalize(image, label):\n        norm_image = image / 255.0\n        return (norm_image, label)\n\n    train_set_normalized = train_set.map(normalize, num_parallel_calls=AUTOTUNE)\n    valid_set_normalized = valid_set.map(normalize, num_parallel_calls=AUTOTUNE)\n\n    # Load the ResNet model\n    base_model = hub.KerasLayer(\n        MODEL_URL,\n        trainable=trainable,\n    )\n\n    model = Sequential(\n        [layers.MaxPool2D(), base_model, layers.Dense(3, activation=\"softmax\")]\n    )\n\n    # Compilation of the model\n    optimizer_ = optimizers.Adam(learning_rate=learning_rate)\n    metrics_ = [\n        \"accuracy\",\n    ]\n    \n    loss_ = \"sparse_categorical_crossentropy\"\n\n    model.compile(optimizer=optimizer_, metrics=metrics_, loss=loss_)\n\n    # Callbacks\n    exponential_decay_fn = ml_learning_rate.exponential_decay_with_warmup(\n        lr_start=learning_rate,\n        lr_max=learning_rate * 10,\n        lr_min=learning_rate / 10,\n    )\n    lr_scheduler_cb = callbacks.LearningRateScheduler(exponential_decay_fn)\n\n    folder_logs = tf.io.gfile.join(\n        \"..\", \"..\", \"..\", \"reports\", \"logs\", \"chapter_14\", \"lung_colon_histopathology\"\n    )\n    logdir = get_logdir(path_folder=folder_logs)\n    tensorboard_cb = callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n\n    model_path = tf.io.gfile.join(MODEL_PATH)\n    model_checkpoint_cb = callbacks.ModelCheckpoint(\n        filepath=model_path, save_best_only=True\n    )\n\n    early_stopping_cb = callbacks.EarlyStopping(patience=5)\n\n    callbacks_list = [\n        lr_scheduler_cb,\n        tensorboard_cb,\n        model_checkpoint_cb,\n        early_stopping_cb,\n    ]\n\n    # Train the model\n    model.fit(\n        train_set_normalized,\n        validation_data=valid_set_normalized,\n        epochs=epochs,\n        callbacks=callbacks_list,\n    )\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_model(\n    train_set,\n    valid_set,\n    test_set,\n    epochs=100,\n    learning_rate=5e-4,\n    trainable=False,\n    lr_function=None\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The result of the training is shown in the images below:\n\n<h4 align=\"center\">Epochs vs Accuracy</h4>\n\n<img src=\"https://storage.googleapis.com/mmenendezg-ml-bucket/models/lung_colon_histopathological/training_images/lung_colon_epoch_accuracy.png\" alt=\"Epochs Vs Accuracy\" style=\"width: 70%; margin-left: 15%; margin-right: 15%\"/>\n\nIn the chart above we see the evolution of the accuracy of the model. We can see in blue the `valid_accuracy`, and in red the `train_accuracy`. At the beginning the `valid_accuracy` is better, however after epoch 11-12 the `train_accuracy` starts improving. This is an expected behavior when training a model, the accuracy will be higher with the training data since that is the dataset used to tune the weights of the model.\n\nAfter the epoch 11-12 the gap between the `train_accuracy` and the `valid_accuracy` is a little higher, but this decreases with the epochs. \n\n<h4 align=\"center\">Epochs vs Loss</h4>\n\n<img src=\"https://storage.googleapis.com/mmenendezg-ml-bucket/models/lung_colon_histopathological/training_images/lung_colon_epoch_loss.png\" alt=\"Epochs Vs Accuracy\" style=\"width: 70%; margin-left: 15%; margin-right: 15%\"/>\n\nIn the second chart we see the evolution of the loss of the model. The code of color keeps the same, being blue for the `valid_loss` and red for the `train_loss`. The tendency is the same, and we can observe the same phenomenon that with the accuracy. \n\nOverall, the result of the model is a model that generalizes well, that is not overfitting the training dataset or underfitting it neither. \n\nWe will evaluate the model in the following section.","metadata":{}},{"cell_type":"markdown","source":"## 4. Evaluation","metadata":{}},{"cell_type":"markdown","source":"This section has as objective the evaluation of the model using different metrics and the confusion matrix.\n\nTensorflow offers the `evaluate()` method with the `Model` object. In this case, the method returns the `accuracy` and the `loss`, since only one metric was specified. \n\nHowever, `tf.keras.metrics` offers the class `Precision` and the class `Recall`. We will use them to evaluate the model and plot the confusion matrix. ","metadata":{}},{"cell_type":"code","source":"def normalize(image, label):\n    \"\"\"\n    Normalize image by dividing each pixel by 255.0\n    \n    Args:\n        image (ndarray): Image data in numpy array format\n        label (int): Image label\n    \n    Returns:\n        tuple: Normalized image in numpy array format and label\n    \"\"\"\n    norm_image = image / 255.0\n    return (norm_image, label)\n\n\ndef plot_confusion_matrix(confusion_matrix, precision, recall, labels=NAME_CLASSES):\n    \"\"\"\n    Plot confusion matrix using seaborn heatmap.\n    \n    Args:\n        confusion_matrix (ndarray): Confusion matrix in numpy array format\n        precision (float): Precision score\n        recall (float): Recall score\n        labels (list, optional): List of class labels, defaults to NAME_CLASSES\n    \"\"\"\n    plt.figure(figsize=(5, 5))\n    sns.heatmap(\n        confusion_matrix,\n        annot=True,\n        cmap=\"flare\",\n        cbar=False,\n        fmt=\".2f\",\n        xticklabels=labels,\n        yticklabels=labels,\n    )\n    title_plot = f\"Confusion Matrix\\nPrecision: {precision * 100:.2f}%\\nRecall: {recall * 100:.2f}%\"\n    plt.xticks(rotation=60)\n    plt.yticks(rotation=60)\n    plt.ylabel(\"Labels\", weight=\"bold\")\n    plt.xlabel(\"Predictions\", weight=\"bold\")\n    plt.title(title_plot)\n\n\ndef create_confusion_matrix(model_path, dataset):\n    \"\"\"\n    Create a confusion matrix for a model given a dataset.\n    \n    Args:\n        model_path (str): File path of the model.\n        dataset (tf.data.Dataset): A dataset containing the images and labels\n    \"\"\"\n    # Load the model\n    model = tf.keras.models.load_model(model_path)\n    # Preprocess the images to predict\n    dataset_normalized = dataset.map(normalize, num_parallel_calls=AUTOTUNE)\n    # Separate the images and the labels\n    images = dataset_normalized.map(\n        lambda image, label: image, num_parallel_calls=AUTOTUNE\n    )\n    labels = dataset_normalized.map(\n        lambda image, label: label, num_parallel_calls=AUTOTUNE\n    )\n    labels = [label.numpy() for label in labels.unbatch()]\n    # Create the confusion matrix with the predicted labels\n    predictions = model.predict(images, verbose=0)\n    predicted_classes = tf.argmax(predictions, axis=1).numpy()\n    # Calculate the precision and recall of the model\n    precision_metric = metrics.Precision()\n    precision_metric.update_state(labels, predicted_classes)\n    precision = precision_metric.result().numpy()\n    \n    recall_metric = metrics.Recall()\n    recall_metric.update_state(labels, predicted_classes)\n    recall = recall_metric.result().numpy()\n    \n    # Create the confusion matrix\n    conf_matrix = tf.math.confusion_matrix(labels, predicted_classes)\n    conf_matrix = conf_matrix.numpy() / conf_matrix.numpy().sum(axis=1)[:, np.newaxis]\n    \n    # Plot the results\n    plot_confusion_matrix(conf_matrix, precision, recall)\n\n\ndef evaluate_model(model_path, test_set):\n    \"\"\"\n    Evaluate a model given a test set.\n    \n    Args:\n        model_path (str): File path of the model.\n        test_set (tf.data.Dataset): A dataset containing the test images and labels\n    \"\"\"\n    # Load the model\n    model = tf.keras.models.load_model(model_path)\n    evaluations = model.evaluate(test_set, verbose=0)\n    print(\n        f\"The evaluation of the model is the following:\\n\\tLoss: {evaluations[0]:.2f}\\n\\tAccuracy: {evaluations[1] * 100:.2f}%\"\n    )\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_set_normalized = test_set.map(normalize, num_parallel_calls=AUTOTUNE)\nevaluate_model(MODEL_PATH, test_set_normalized)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"create_confusion_matrix(MODEL_PATH, test_set)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The confusion matrix above is normalized, meaning that it shows the percentage of the predictions made. Having 0.95 and above in the main diagonal shows that this model has good accuracy as calculated by the `evaluate()` method: $97.60\\%$. But also precision and recall are very high.\n\nLet's remember that in this case the model takes images from both organs and generalize to an unified diagnosis, meaning that this model has learned to generalize well. ","metadata":{}},{"cell_type":"markdown","source":"## 5. Making Predictions\n","metadata":{}},{"cell_type":"markdown","source":"The final section of the model is to make predictions on images that the model was not trained on. In this case we will use images from the test set.\n\nThe labels have been removed and the images have not been normalized before being fed to the model.","metadata":{}},{"cell_type":"code","source":"def preprocess_image(image):\n    # Normalize the image\n    norm_image = image / 255.0\n\n    # Add dimension at the beginning\n    norm_image = tf.expand_dims(norm_image, axis=0)\n    return norm_image\n\n\ndef make_prediction(dataset, model_path=MODEL_PATH, n_cols=4):\n    # Load the model\n    model = tf.keras.models.load_model(model_path)\n    # Preprocess the images to predict\n    dataset_normalized = dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n    predictions = model.predict(dataset_normalized, verbose=0)\n    predicted_classes = tf.argmax(predictions, axis=1).numpy()\n\n    # Determine the number of images to show\n    n_images = len(predicted_classes)\n    n_rows = (\n        (n_images // n_cols) if (n_images % n_cols) == 0 else (n_images // n_cols) + 1\n    )\n    idx = 1\n    plt.figure(figsize=(5 * n_cols, 5 * n_rows))\n    for image, label in zip(dataset_normalized, predicted_classes):\n        prob_prediction = predictions[idx - 1][label] * 100\n        plt.subplot(n_rows, n_cols, idx)\n        plt.imshow(image[0].numpy())\n        plt.title(f\"Type Tissue: {TYPE_TISSUE[label]}\\nProbability: {prob_prediction:.2f}%\")\n        plt.axis(\"off\")\n        idx += 1","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = (\n    test_set.map(lambda image, label: image, num_parallel_calls=AUTOTUNE)\n    .unbatch()\n    .shuffle(2500)\n    .take(15)\n)\nmake_prediction(preds)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see that for most of the images, the model generalizes pretty well. The only case where the model is uncertain is the last image. \n\nHaving more images to train the model would be optimal, and in this case maybe doing more augmentation (changing the contrast, or rotating the image) would benefit the model. ","metadata":{}}]}